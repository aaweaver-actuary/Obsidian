* recall the [[2. Overview of Statistical Learning#2.1 Linear Regression|linear model]]:
$$
Y=\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_pX_p+\epsilon
$$
* in the lectures that follow, we consider some approaches for extending the linear model framework
* in the lectures covering [[7. Moving Beyond Linearity|chapter 7 of the text]], we generalize the linear model to accommodate *non-linear*, but still *additive* relationships between the predictors and the response
* in the lectures covering chapter 8 we consider even more general *non-linear* models

### 6.1 In praise of linear models!

* despite its simplicity, the linear model has distinct advantages in terms of its *interpretability* and often shows good *predictive performance*
* hence we discuss in this lecture some ways in which the simple linear model can be improved
    * by replacing ordinary least squares (OLS) with a some alternative fitting procedures

### 6.2 Why consider alternatives to OLS?

#### prediction accuracy
* OLS will perform well if $n$ is large relative to $p$ (number of predictors)
* when $p > n$, look at alternatives to control variance of the estimates
	* in this case least squares is not even defined
#### model interpretability:
* by removing irrelevant features
	* eg by setting their coefficients to zero
* we obtain a model that is easier to interpret
	* we will present some approaches for automatically performing *feature selection*
	    * ie for automatically identifying a subset of the $p$ predictors that we believe to be related to the response

### 6.3 Three classes of methods:

1. *subset selection* - we identify a subset of the $p$ predictors that we believe to be related to the response
    * we then fit a model using least squares on the reduced set of variables
2. *shrinkage* - we fit a model involving all $p$ predictors, but the estimated coefficients are shrunken towards zero relative to the least squares estimates
    * this shrinkage (aka *regularization*) has the effect of reducing variance of the fitted coefficients, and can also perform variable selection
    * shrinkage methods can also be viewed as *constrained* versions of least squares
3. *dimension reduction* - we project the $p$ predictors into a $M$-dimensional subspace, where $M < p$
    * this is achieved by computing $M$ different linear combinations, or *projections*, of the variables
    * then these $M$ projections are used as predictors to fit a linear regression model by least squares
    * we will look at two approaches: *principal components regression* and *partial least squares*

### 6.4 Subset selection

*best subset and stepwise model selection procedures*
#### 6.4.1 Best subset selection
1. Let $\mathcal{M}_0$ denote the *null model*, which contains no predictors. (This model simply predicts the sample mean for each observation.)
2. For $k=1,2,\cdots,p$:
    * Fit all $\binom{p}{k}$ models that contain exactly $k$ predictors.
    * Pick the best among these $\binom{p}{k}$ models, and call it $M_k$. Here *best* is defined as having the smallest RSS, or equivalently largest $R^2$.
3. Select a single best model from among $\mathcal{M}_0, \mathcal{M}_1, \cdots, \mathcal{M}_p$ using cross-validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$.

##### Extensions to other models

* we presented best subset selection for least squares linear regression
    * but the same framework can be applied to other models as well
* the **deviance** is negative twice the maximized log-likelihood
    * it is a measure of model fit
    * it plays the role of RSS for a broader class of models

### 6.5 Stepwise selection

* for computational reasons, best subset selection cannot be applied with very large $p$
* best subset selection may also suffer from statistical problems when $p$ is large
    * it will tend to choose models that have too many variables
    * larger search space => larger chance of overfitting
* an enormous search space can lead to overfitting
* we can use stepwise selection instead
    * stepwise methods explore a far more restricted set of models
* big idea is that you restrict the number of models examined by best subset selection
    * by **imposing a threshold** on the variable entry and exit
    * restricting the number of models examined lowers the risk of overfitting/the risk of selecting models that include irrelevant variables
* forward and backward stepwise selection are two closely related variants of this idea

#### 6.5.1 Forward stepwise selection

* begin with a model containing no predictors
* we then add predictors to the model, one-at-a-time, until all of the predictors are in the model
* at each step, the variable that gives the greatest additional improvement to the fit is added to the model
* we stop once no significant improvement can be made

##### In detail - forward stepwise selection
1. Let $\mathcal{M}_0$ denote the null model, which contains no predictors (ie just the intercept)
2. For $k=0,\dots,p-1$:
    1. Consider all $p-k$ models that augment the predictors in $\mathcal{M}_k$ with one additional predictor
    2. Choose the best among these $p-k$ models, and call it $\mathcal{M}_{k+1}$
3. Select a single best model from among $\mathcal{M}_0,\dots,\mathcal{M}_p$ using cross-validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$

* computational advantage over best subset selection
    * best subset selection involves fitting $2^p$ models for $p predictors
        * really only feasible for $p < 40$, but even authors only recommend for $p < 10$
    * forward stepwise selection considers $1 + p(p+1)/2$ models
        
* it is not guaranteed to find the best possible model containing a subset of the p predictors
    * in fact, RSS can be no lower than that obtained by best subset selection, but it can be higher
    * this is due to the correlation between predictors
  
#### 6.5.2 Backward stepwise selection

* like forward stepwise selection, but in reverse:
    * start with all predictors in the model
    * remove the least useful predictor one at a time

##### Details - backward stepwise selection

1. Let $\mathcal{M}_p$ denote the full model, which contains all $p$ predictors.
2. For $k = p, p-1, \dots, 1$:
    * Consider all $k$ models that contain all but one of the predictors in $\mathcal{M}_k$, for a total of $k-1$ predictors.
    * Choose the best among these $k$ models, and call it $\mathcal{M}_{k-1}$.
    * Best is defined as having the smallest RSS or highest $R^2$.
3. Select a single best model from among $\mathcal{M}_0, \dots, \mathcal{M}_p$ using cross-validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$.

##### More on backward stepwise selection

* considers $1 + p(p+1)/2$ models
* like forward stepwise selection, it is not guaranteed to find the best possible model containing a subset of the p predictors
    * in fact, RSS can be no lower than that obtained by best subset selection, but it can be higher
    * this is due to the correlation between predictors
* backward selection requires that the number of samples $n$ is larger than the number of predictors $p$, or else the full model cannot be fit
    * in contrast, **forward selection** can be used even when $n < p$, and so is the **only viable subset method when $p > n$**

### 6.6 Choosing the optimal model

* the model containing all the predictors will always have the smallest training error
    * this is because the training error decreases as more variables are added to the model
* we want to choose a model with low *test* error, not low *training* error
    * recall that training error is not a good estimate of test error
    * therefore RSS and $R^2$ are not suitable for selecting the best model among a collection of models with different numbers of predictors
    * we can estimate test error using CV, AIC, BIC, or adjusted $R^2$
##### what do we need?
a way to estimate the test error associated with a given model, in order to select the best model

### 6.7 Estimating test error: two approaches
* we can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting
    * this adjustment is estimated using the same training data
    * this approach is called **shrinkage**
* we can directly estimate the test error, using either a validation set approach or a cross-validation approach
    * this approach is called **subset selection**

### 6.8 $C_p$, AIC, BIC, and adjusted $R^2$

* these techniques adjust the training error for the model size, and can be used to select among a set of models with different numbers of variables
* the next figure displays $C_p$, AIC, BIC, and adjusted $R^2$ for all models ranging from the intercept-only model containing no predictors, to the full model containing all $p$ predictors


#### 6.8.1 Mallows' $C_p$
* this is an statistic that adjusts the training RSS by adding a penalty term that is larger when $p$ is larger
    * used to approximate test error
$$
C_p = \frac{1}{n}(RSS + 2d\hat{\sigma}^2)
$$
where  
* $d$ is the total number of parameters in the model
* $\hat{\sigma}^2$ is an estimate of the variance of the error $\epsilon$ associated with each response measurement

#### 6.8.2 Akaike information criterion (AIC)

* defined for a large class of models fit by maximum likelihood
$$
\text{AIC} = -2\log L + 2d
$$
where
* $L$ is the maximized value of the likelihood function for the estimated model

**NOTE** that in the case of a linear model with Gaussian errors, maximum likelihood and least squares are the same thing, up to a constant additive term:
$$
-2\ell = \frac{\text{RSS}}{\hat{\sigma}^2}
$$
* this means that the AIC statistic is equivalent to $C_p$ up to an additive constant

#### 6.8.3 Bayesian information criterion (BIC)

$$
\begin{align}
\text{BIC} =& \frac{1}{n}\left(\text{RSS} + \log(n)d\hat{\sigma}^2\right) \\
=& -2 \ell + \log(n)d
\end{align}
$$

* like $C_p$, BIC will tend to take on a small value for a model with a low test error, and so generally we select the model for which BIC is smallest
* notice that BIC replaces the $2d\hat{\sigma}^2$ term in $C_p$ with a $\log(n)d\hat{\sigma}^2$ term
    * this is a stronger penalty than $C_p$, and so BIC tends to favor smaller models than $C_p$
    * in particular, when $n \geq 8$, $\log(n) \geq 2$, so BIC places a heavier penalty on the number of parameters
    * this is sometimes referred to as a *Bayesian Occam's razor*

#### 6.8.4 Adjusted $R^2$

* for a least squares model with $d$ variables, the adjusted $R^2$ statistic is calculated as
$$
\text{Adjusted } R^2 = 1 - \frac{\text{RSS}/(n-d-1)}{\text{TSS}/(n-1)}
$$

* here $\text{TSS} = \sum_{i=1}^n(y_i - \bar{y})^2$ is the total sum of squares
* unlike $C_p$, AIC, BIC, for which a *small* value indicates a model with a low test error, a *large* value of adjusted $R^2$ indicates a model with a low test error
* maximizing the adjusted $R^2$ is equivalent to minimizing $\text{RSS} / (n-d-1)$
    * while $\text{RSS}$ is guaranteed to decrease as more variables are added to the model, $\text{RSS} / (n-d-1)$ may increase or decrease, due to the presence of the $(n-d-1)$ term in the denominator
* unlike the $R^2$ statistic, the adjusted $R^2$ statistic *pays a price* for the inclusion of unnecessary variables in the model

#### 6.8.5 Validation and cross-validation

* each of the procedures returns a sequence of models $\mathcal{M}_0, \dots, \mathcal{M}_p$ indexed by the number of variables
    * our job here is to select a single model from among these
* we compute the vaidation set error or the cross-validation error for each model, and then select the model for which the resulting estimated test error is smallest
* this procedure has an advantage relative to AIC, BIC, $C_p$, and adjusted $R^2$, in that it provides a direct estimate of the test error, and *does not require an estimate of the error variance $\hat{\sigma}^2$*
* it can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom $d$ or hard to estimate the error variance $\hat{\sigma}^2$
* **DON'T CHOOSE THE SMALLEST CV ERROR, CHOOSE THE SIMPLEST MODEL *WITHIN ONE STANDARD DEVIATION* OF THE SMALLEST MEAN CV ERROR**

### 6.9 Shrinkage methods

* subset selection methods use least squares to fit a linear model that contains a subset of the predictors
* as an alternative, we can fit a model containing all $p$ predictors using a technique that *constrains* or *regularizes* the coefficient estimates, or equivalently, that *shrinks* the coefficient estimates towards zero
* it may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance

#### 6.9.1 Ridge regression
* recall that the least squares fitting procedure estimates $\beta_0, \beta_1, \dots, \beta_p$ using the values that minimize
$$
\text{RSS} = \sum_{i=1}^n \left( y_i - \beta_0 -\sum_{j=1}^p \beta_j x_{ij} \right)^2
$$
* in contrast, the ridge regression coefficient estimates $\hat{\beta}^R = \hat{\beta}^R(\lambda)$ are the values that minimize
$$
\begin{align}
\sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 +& \lambda \sum_{j=1}^p \beta_j^2 \\
= \text{RSS } +& \lambda \sum_{j=1}^p \beta_j^2
\end{align}
$$
* here $\lambda \geq 0$ is a *tuning parameter*, to be determined separately
* as with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small
* however, the second term, $\lambda \sum_j\beta_j^2$, called a *shrinkage penalty*, is small when $\beta_1, \dots, \beta_p$ are close to zero, and so it has the effect of shrinking the estimates of $\beta_j$ towards zero
* the tuning parameter $\lambda$ serves to control the relative impact of these two terms on the regression coefficient estimates
* selecting a good value for $\lambda$ is critical
    * cross-validation can be used to make this choice

#### 6.9.2 Ridge regression: scaling of predictors

* the standard least squares coefficient estimates are *scale equivariant*: 
    * multiplying $X_j$ by a constant $c$ simply leads to a scaling of the least squares coefficient estimates by a factor of $1/c$
    * in other words, regardless of how the $j$th predictor is scaled, the least squares coefficient estimates will remain the same
* in contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant, due to the sum of squared coefficients term in the penalty part of the ridge regression objective function
* therefore it is best to apply ridge regression after standardizing the predictors

#### 6.9.3 Ridge regression: centering of predictors

* ridge regression also has an intercept term that is not shrunk
* therefore, ridge regression coefficient estimates are not equivariant to the addition of a constant to each predictor, and so it is best to apply ridge regression after *centering* the predictors to have mean zero
* in this case, the intercept will equal the mean of the response, and so can be ignored in making predictions

#### 6.9.4 Why does ridge regression improve over least squares?

* the bias-variance tradeoff states that the expected test MSE, for a given value $x_0$, can always be decomposed as follows:
$$
\begin{align}
\text{E}(y_0 - \hat{f}(x_0))^2 &= \text{Var}(\hat{f}(x_0)) + [\text{Bias}(\hat{f}(x_0))]^2 + \text{Var}(\epsilon) \\
&= \text{Var}(\hat{f}(x_0)) + [\text{Bias}(\hat{f}(x_0))]^2 + \sigma^2
\end{align}
$$
* here $\text{Var}(\hat{f}(x_0))$ represents the amount by which $\hat{f}$ would change if we estimated it using a different training data set
* $\text{Bias}(\hat{f}(x_0))$ represents the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model
* $\sigma^2$ is the irreducible error, which is the variance of the error term $\epsilon$
* as we increase the $\lambda$ parameter, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias
    * if a given model had a small variance, then the decrease in variance due to shrinking the coefficients will be small
    * on the other hand, if a given model had a large variance, then the decrease in variance could be dramatic and offset the increase in bias


### 6.10 The Lasso

* [[6. Linear Model Selection and Regularization#6.9.1 Ridge regression|Ridge regression]] has one obvious disadvantage: it includes all $p$ predictors in the final model
* the **lasso** is a relatively recent alternative to ridge regression that overcomes this disadvantage
* lasso coefficients $\hat{\beta}^L = \hat{\beta}^L(\lambda)$ minimize
$$
\begin{align}
\sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 +& \lambda \sum_{j=1}^p |\beta_j| \\
= \text{RSS } +& \lambda \sum_{j=1}^p |\beta_j|
\end{align}
$$
* lasso uses an $\ell_1$ penalty instead of an $\ell_2$ penalty
* as with ridge regression, the lasso shrinks the coefficient estimates towards zero
* however, in the case of the lasso, the $\ell_1$ penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter $\lambda$ is sufficiently large
* hence, much like best subset selection, the lasso performs *variable selection* and yields *sparse* models with only a subset of the variables
* as with ridge regression, selecting a good value for $\lambda$ is critical
    * cross-validation can be used to make this choice

### 6.11 Conclusions
* neither ridge nor lasso will universally dominate the other
* in general, one might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero
* however, the number of predictors that is related to the response is never known a priori for real data sets
* a technique such as cross-validation can be used in order to determine which approach is better on a particular data set

### 6.12 Selecting the tuning parameter $\lambda$ for ridge regression and the lasso

* it is important
* as for subset selection we require a method to determine which of the models under consideration is best
* that is, we require a way to select the tuning parameter $\lambda$
* **cross-validation** provides a simple way to tackle this problem
    * we choose a grid of $\lambda$ values, and compute the cross-validation error for each value of $\lambda$
    * we then select the tuning parameter value for which the cross-validation error is smallest
    * finally, the model is re-fit using all of the available observations and the selected value of $\lambda$

### 6.13 Dimension reduction methods

* the methods discussed so far in this chapter have involved fitting a linear model involving $p$ predictors using either least squares or a shrinkage approach
* we now consider some approaches that transform the predictors and then fit a least squares model using the transformed variables
    * these approaches will be called **dimension reduction methods**
* the goal of dimension reduction is to reduce the $p$ predictors to a $M$-dimensional subspace, where $M < p$

#### 6.13.1 Details

* Let $Z_1, Z_2, \dots, Z_M$ represent $M < p$ linear combinations of our original $p$ predictors. That is,
$$
Z_m = \sum_{j=1}^p \phi_{jm}X_j
$$
* where $\phi_{1m}, \phi_{2m}, \dots, \phi_{pm}$ are constants
* we can then fit the following linear regression model using least squares:
$$
y_i = \theta_0 + \sum_{m=1}^M \theta_m z_{im} + \epsilon_i, \quad i = 1, \dots, n
$$
* note that in the model, the regression coefficients are given by $\theta_0, \theta_1, \dots, \theta_M$
    * if the constants $\phi_{jm}$ are chosen wisely, then the $M$-dimensional response $Z_m$ may be much simpler to model than the original $p$-dimensional response $X_j$
* note that, from the definition of $Z_m$, we have
$$
\begin{align}
\sum_{m=1}^M \theta_m z_{im}  =& \sum_{m=1}^M \theta_m \sum_{j=1}^p \phi_{jm} x_{ij} \\
=& \sum_{j=1}^p \sum_{m=1}^M \theta_m \phi_{jm} x_{ij} \\
=& \sum_{j=1}^p \beta_j x_{ij} \\
\text{where } \beta_j =& \sum_{m=1}^M \theta_m \phi_{jm}
\end{align}
$$
* thus the model is equivalent to the least squares regression of $y$ onto $X_1, X_2, \dots, X_p$
    * it can be considered a special case of the linear model in which the predictors are linear combinations of the original predictors
* the dimension reduction approach is most useful in the case where $M < p$, that is, when we have reduced the problem to a smaller number of predictors
    * in this case, the least squares approach is more stable
    * can win in the bias-variance tradeoff

### 6.14 Principal Components Regression

* here we apply principal components analysis (PCA) to define the linear combinations of the predictors, for use in our regression model
* the first principal component that is that (normalized) linear combination of the variables that has the largest variance
    * the second principal component has largest variance out of all linear combinations that are uncorrelated with the first principal component
    * and so on
* hence with many correlated original variables, we replace them with a small set of principal components that capture their joint variation

### partial least squares

* principal components regression identifies linear combinations or *directions* that best represent the predictors $X_1, X_2, \dots, X_p$
* these directions are identified in an *unsupervised* way, since the repsonse $Y$ plays no role in determining them
* consequently, PCR suffers from a potentially serious drawback:
    * there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response
* like PCR, partial least squares (PLS) is also a dimension reduction method
* PLS first identifies a new set of features $Z_1, Z_2, \dots, Z_M$ that are linear combinations of the original features, and then fits the model
* unlike PCR, PLS identifies these new features in a *supervised* way - that is, it makes use of the response $Y$
    * PLS identifies new features that not only approximate the old features well, but also that are related to the response
* roughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors

#### details of PLS

* standardize the $p$ predictors to have mean zero and standard deviation one
* the first PLS direction $Z_1$ is found by setting each $\phi_{1j}$ equal to the coefficient from the simple linear regression of $Y$ onto $X_j$
* one can show that this coefficient is proportional to the correlation between $Y$ and $X_j$
* thus in computing $Z_1 = \sum_{j=1}^p \phi_{1j} X_j$, PLS places the highest weight on the variables that are most strongly related to the response
* subsequent directions are found by taking the residuals from the regression of each $X_j$ on $Z_1, Z_2, \dots, Z_{m-1}$