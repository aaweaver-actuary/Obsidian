[Kullback–Leibler Divergence Wikipedia](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)

[[Kullback–Leibler Divergence]] measures how one probability distribution diverges from a second, reference distribution. It quantifies the "distance" in terms of #bit's or #nat's. Often used in 

[[Definition of Kullback-Leibler Divergence]]
[[Properties of Kullback-Leibler Divergence]]
[[How to evaluate a model with Kullback-Leibler divergence]]
