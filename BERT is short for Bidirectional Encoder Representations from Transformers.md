---
annotation-target::https://arxiv.org/pdf/1810.04805.pdf
---

[[BERT is short for Bidirectional Encoder Representations from Transformers|BERT]] #transformer [[Deep Learning]] [[Natural Language Processing (NLP)]]

[[BERT is a Swiss-Army knife that can be used on a wide variety of nlp tasks]]

[[DistilBert is 60% faster than BERT while maintaining 95% of BERT's accuracy]]

[[Bert is trained with a masked language model]]

[[The transformer architecture makes it possible to parallelize masked language training extremely efficiently]]



>%%
>```annotation-json
>{"created":"2023-08-22T23:58:52.642Z","updated":"2023-08-22T23:58:52.642Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":444,"end":527},{"type":"TextQuoteSelector","exact":"BERT is designed to pre-train deep bidirectional representations fromunlabeled text","prefix":" 2018a; Rad-ford et al., 2018), ","suffix":" by jointly conditioning on both"}]}]}
>```
>%%
>*%%PREFIX%%2018a; Rad-ford et al., 2018),%%HIGHLIGHT%% ==BERT is designed to pre-train deep bidirectional representations fromunlabeled text== %%POSTFIX%%by jointly conditioning on both*
>%%LINK%%[[#^t2vlkgge72j|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^t2vlkgge72j



>%%
>```annotation-json
>{"created":"2023-08-22T23:59:47.824Z","updated":"2023-08-22T23:59:47.824Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":615,"end":693},{"type":"TextQuoteSelector","exact":"pre-trained BERT model can be fine-tuned with just one additional output layer","prefix":"n all layers. As a re-sult, the ","suffix":"to create state-of-the-art model"}]}]}
>```
>%%
>*%%PREFIX%%n all layers. As a re-sult, the%%HIGHLIGHT%% ==pre-trained BERT model can be fine-tuned with just one additional output layer== %%POSTFIX%%to create state-of-the-art model*
>%%LINK%%[[#^qs3e9gpmep|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^qs3e9gpmep


>%%
>```annotation-json
>{"created":"2023-08-23T00:00:17.051Z","updated":"2023-08-23T00:00:17.051Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":1290,"end":1399},{"type":"TextQuoteSelector","exact":"Language model pre-training has been shown tobe effective for improving many natural languageprocessing tasks","prefix":"lute improvement).1 Introduction","suffix":" (Dai and Le, 2015; Peters et al"}]}]}
>```
>%%
>*%%PREFIX%%lute improvement).1 Introduction%%HIGHLIGHT%% ==Language model pre-training has been shown tobe effective for improving many natural languageprocessing tasks== %%POSTFIX%%(Dai and Le, 2015; Peters et al*
>%%LINK%%[[#^h1jszbh16lo|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^h1jszbh16lo


>%%
>```annotation-json
>{"created":"2023-08-23T00:01:47.327Z","updated":"2023-08-23T00:01:47.327Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":1649,"end":1700},{"type":"TextQuoteSelector","exact":"aim to predict the re-lationships between sentences","prefix":"Dolanand Brockett, 2005), which ","suffix":" by analyzing themholistically, "}]}]}
>```
>%%
>*%%PREFIX%%Dolanand Brockett, 2005), which%%HIGHLIGHT%% ==aim to predict the re-lationships between sentences== %%POSTFIX%%by analyzing themholistically,*
>%%LINK%%[[#^lppyxqk4z5h|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^lppyxqk4z5h


>%%
>```annotation-json
>{"created":"2023-08-23T00:01:56.456Z","updated":"2023-08-23T00:01:56.456Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":1704,"end":1731},{"type":"TextQuoteSelector","exact":"analyzing themholistically,","prefix":"ationships between sentences by ","suffix":" as well as token-level tasks su"}]}]}
>```
>%%
>*%%PREFIX%%ationships between sentences by%%HIGHLIGHT%% ==analyzing themholistically,== %%POSTFIX%%as well as token-level tasks su*
>%%LINK%%[[#^wf36rr9czqo|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^wf36rr9czqo


>%%
>```annotation-json
>{"created":"2023-08-23T00:02:04.458Z","updated":"2023-08-23T00:02:04.458Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":1743,"end":1815},{"type":"TextQuoteSelector","exact":"token-level tasks such asnamed entity recognition and question answering","prefix":"ng themholistically, as well as ","suffix":",where models are required to pr"}]}]}
>```
>%%
>*%%PREFIX%%ng themholistically, as well as%%HIGHLIGHT%% ==token-level tasks such asnamed entity recognition and question answering== %%POSTFIX%%,where models are required to pr*
>%%LINK%%[[#^qp3rtpdqfhn|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^qp3rtpdqfhn


>%%
>```annotation-json
>{"created":"2023-08-23T07:49:45.015Z","updated":"2023-08-23T07:49:45.015Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":1500,"end":1521},{"type":"TextQuoteSelector","exact":"sentence-level tasks ","prefix":" and Ruder,2018). These include ","suffix":"such asnatural language inferenc"}]}]}
>```
>%%
>*%%PREFIX%%and Ruder,2018). These include%%HIGHLIGHT%% ==sentence-level tasks== %%POSTFIX%%such asnatural language inferenc*
>%%LINK%%[[#^9q286733cu8|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^9q286733cu8


>%%
>```annotation-json
>{"text":"1. feature-based\n2. fine-tuning","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":1962,"end":2058},{"type":"TextQuoteSelector","exact":"two existing strategies for apply-ing pre-trained language representations to down-stream tasks:","prefix":"jpurkar et al., 2016).There are","suffix":"feature-based and fine-tuning."}]}],"created":"2023-08-23T07:50:16.184Z","updated":"2023-08-23T07:50:16.184Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf"}
>```
>%%
>*%%PREFIX%%jpurkar et al., 2016).There are%%HIGHLIGHT%% ==two existing strategies for apply-ing pre-trained language representations to down-stream tasks:== %%POSTFIX%%feature-based and fine-tuning.*
>%%LINK%%[[#^6a1nkzdkdd8|show annotation]]
>%%COMMENT%%
>1. feature-based
>2. fine-tuning
>%%TAGS%%
>
^6a1nkzdkdd8


>%%
>```annotation-json
>{"created":"2023-08-23T09:59:43.935Z","text":"**feature-based:**\n* use task-specific architectures\n    * include pre-trained representations as add'l features\n","updated":"2023-08-23T09:59:43.935Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":2090,"end":2254},{"type":"TextQuoteSelector","exact":"Thefeature-based approach, such as ELMo (Peterset al., 2018a), uses task-specific architectures thatinclude the pre-trained representations as addi-tional features.","prefix":" feature-based and fine-tuning. ","suffix":" The fine-tuning approach, such "}]}]}
>```
>%%
>*%%PREFIX%%feature-based and fine-tuning.%%HIGHLIGHT%% ==Thefeature-based approach, such as ELMo (Peterset al., 2018a), uses task-specific architectures thatinclude the pre-trained representations as addi-tional features.== %%POSTFIX%%The fine-tuning approach, such*
>%%LINK%%[[#^y1wpjdmien|show annotation]]
>%%COMMENT%%
>**feature-based:**
>* use task-specific architectures
>    * include pre-trained representations as add'l features
>
>%%TAGS%%
>#feature-based, #ELMo
^y1wpjdmien


>%%
>```annotation-json
>{"created":"2023-08-23T10:01:19.386Z","text":"**fine-tuning:**\n* minimal task-specific parameters\n* trained on downstream tasks by simply fine-tuning all pretrained parameters\n* this is what GPT is","updated":"2023-08-23T10:01:19.386Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":2255,"end":2494},{"type":"TextQuoteSelector","exact":"The fine-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-specific parameters, and is trained on thedownstream tasks by simply fine-tuning all pre-trained parameters.","prefix":"ations as addi-tional features. ","suffix":" The two approaches share thesam"}]}]}
>```
>%%
>*%%PREFIX%%ations as addi-tional features.%%HIGHLIGHT%% ==The fine-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-specific parameters, and is trained on thedownstream tasks by simply fine-tuning all pre-trained parameters.== %%POSTFIX%%The two approaches share thesam*
>%%LINK%%[[#^xituzy7wohf|show annotation]]
>%%COMMENT%%
>**fine-tuning:**
>* minimal task-specific parameters
>* trained on downstream tasks by simply fine-tuning all pretrained parameters
>* this is what GPT is
>%%TAGS%%
>#fine-tune, #gpt
^xituzy7wohf


>%%
>```annotation-json
>{"created":"2023-08-23T10:02:48.831Z","text":"two approaches:\n\n**same objective function** - unidirectional language models","updated":"2023-08-23T10:02:48.831Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":2523,"end":2613},{"type":"TextQuoteSelector","exact":"same objective function during pre-training, wherethey use unidirectional language models ","prefix":"rs. The two approaches share the","suffix":"to learngeneral language represe"}]}]}
>```
>%%
>*%%PREFIX%%rs. The two approaches share the%%HIGHLIGHT%% ==same objective function during pre-training, wherethey use unidirectional language models== %%POSTFIX%%to learngeneral language represe*
>%%LINK%%[[#^7vgmqxtrflh|show annotation]]
>%%COMMENT%%
>two approaches:
>
>**same objective function** - unidirectional language models
>%%TAGS%%
>#objective-function, #unidirectional-language-model
^7vgmqxtrflh


>%%
>```annotation-json
>{"created":"2023-08-23T10:04:54.660Z","text":"**argument:** current techniques restrict the power of the pre-trained representations","updated":"2023-08-23T10:04:54.660Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":2654,"end":2784},{"type":"TextQuoteSelector","exact":"We argue that current techniques restrict thepower of the pre-trained representations, espe-cially for the fine-tuning approaches.","prefix":"eneral language representations.","suffix":" The ma-jor limitation is that s"}]}]}
>```
>%%
>*%%PREFIX%%eneral language representations.%%HIGHLIGHT%% ==We argue that current techniques restrict thepower of the pre-trained representations, espe-cially for the fine-tuning approaches.== %%POSTFIX%%The ma-jor limitation is that s*
>%%LINK%%[[#^u0eyvwi2wiq|show annotation]]
>%%COMMENT%%
>**argument:** current techniques restrict the power of the pre-trained representations
>%%TAGS%%
>
^u0eyvwi2wiq


>%%
>```annotation-json
>{"created":"2023-08-23T10:05:55.352Z","text":"**major limitation of current methods:** unidirectional architecture","updated":"2023-08-23T10:05:55.352Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":2789,"end":2858},{"type":"TextQuoteSelector","exact":"ma-jor limitation is that standard language models areunidirectional,","prefix":"the fine-tuning approaches. The ","suffix":" and this limits the choice of a"}]}]}
>```
>%%
>*%%PREFIX%%the fine-tuning approaches. The%%HIGHLIGHT%% ==ma-jor limitation is that standard language models areunidirectional,== %%POSTFIX%%and this limits the choice of a*
>%%LINK%%[[#^jqfr27t5hqb|show annotation]]
>%%COMMENT%%
>**major limitation of current methods:** unidirectional architecture
>%%TAGS%%
>#unidirectional-language-model
^jqfr27t5hqb


>%%
>```annotation-json
>{"created":"2023-08-23T10:07:05.511Z","updated":"2023-08-23T10:07:05.511Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":3382,"end":3458},{"type":"TextQuoteSelector","exact":"In this paper, we improve the fine-tuning basedapproaches by proposing BERT:","prefix":"te context from both directions.","suffix":" BidirectionalEncoder Representa"}]}]}
>```
>%%
>*%%PREFIX%%te context from both directions.%%HIGHLIGHT%% ==In this paper, we improve the fine-tuning basedapproaches by proposing BERT:== %%POSTFIX%%BidirectionalEncoder Representa*
>%%LINK%%[[#^efldtk3n54c|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^efldtk3n54c


>%%
>```annotation-json
>{"created":"2023-08-23T10:07:27.006Z","text":"**benefit of BERT:** uses masked language model pre-training objective","updated":"2023-08-23T10:07:27.006Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":3514,"end":3650},{"type":"TextQuoteSelector","exact":"BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked lan-guage model” (MLM) pre-training objective,","prefix":"presentations from Transformers.","suffix":" in-spired by the Cloze task (Ta"}]}]}
>```
>%%
>*%%PREFIX%%presentations from Transformers.%%HIGHLIGHT%% ==BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked lan-guage model” (MLM) pre-training objective,== %%POSTFIX%%in-spired by the Cloze task (Ta*
>%%LINK%%[[#^svpd6lrl9g|show annotation]]
>%%COMMENT%%
>**benefit of BERT:** uses masked language model pre-training objective
>%%TAGS%%
>#objective-function
^svpd6lrl9g


>%%
>```annotation-json
>{"created":"2023-08-23T10:09:21.952Z","text":"**point of MLM:** can use context from both the left and the right","updated":"2023-08-23T10:09:21.952Z","document":{"title":"1810.04805.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"https://arxiv.org/pdf/1810.04805.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"https://arxiv.org/pdf/1810.04805.pdf","target":[{"source":"https://arxiv.org/pdf/1810.04805.pdf","selector":[{"type":"TextPositionSelector","start":3966,"end":4046},{"type":"TextQuoteSelector","exact":"MLM ob-jective enables the representation to fuse the leftand the right context,","prefix":"anguage model pre-training, the ","suffix":" which allows us to pre-train a "}]}]}
>```
>%%
>*%%PREFIX%%anguage model pre-training, the%%HIGHLIGHT%% ==MLM ob-jective enables the representation to fuse the leftand the right context,== %%POSTFIX%%which allows us to pre-train a*
>%%LINK%%[[#^2rn8t9wp38|show annotation]]
>%%COMMENT%%
>**point of MLM:** can use context from both the left and the right
>%%TAGS%%
>
^2rn8t9wp38
