[[BERT is short for Bidirectional Encoder Representations from Transformers|BERT]] [[Natural Language Processing (NLP)|NLP]]
[[A masked language model enforces bidirectional learning from text by masking a word in a sentence and forcing BERT to bidirectionally use the words on either side of the covered word to predict the masked word]]
