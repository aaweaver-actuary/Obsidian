- Here we approach the binary target problem in direct way:
	- [[Optimal Separating Hyperplanes]]
- [[Support Vector Classifier]]
	- [[The support vector classifier maximizes a soft margin]]
	- [[Sometimes a linear boundary can fail in a SVM]]
- [[Feature expansion and the SVM]]
- [[The more elegant and controlled way to overcome a failed linear decision boundary in a SVM is through kernels]]
- [[Using SVMs with more than 2 classes]]
- [[Support vector machines vs logistic regression]]

- [[No Free Lunch Theorem]]: with a SVM we saw a way with [[Kernel functions are functions of two variables that compute inner products|kernel functions]] to get a solution in infinite dimensions for free
	- you do pay a price though - you don't get #feature-selection like you do with [[The Lasso for regularization|the lasso]] 