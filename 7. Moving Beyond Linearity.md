* the truth is never linear
* often it is good enough
* when it isn't:
    * polynomial regression
    * step functions
    * splines
    * local regression
    * generalized additive models (GAMs)
    
* these offer a lot of flexibility without losing the ease and interpretability of linear models

# 7.1 Polynomial Regression

$$
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \cdots + \beta_d x_i^d + \epsilon_i
$$
## 7.1.1 Details
* create new variables $X_1 = X$, $X_2 = X^2$, $X_3 = X^3$, etc. and then treat as multiple linear regression
* not really interested in the individual coefficients, but rather the overall shape of the curve at a value $x_0$:
$$

\hat{f}(x_0) = \hat{\beta}_0 + \hat{\beta}_1 x_0 + \hat{\beta}_2 x_0^2 + \hat{\beta}_3 x_0^3 + \cdots + \hat{\beta}_d x_0^d

$$
* since $\hat{f}(x_0)$ is a linear function of the $\hat{\beta}_j$, we can get a simple expression for *pointwise-variances* $\text{Var}(\hat{f}(x_0))$ at any value $x_0$

# 7.2 Piecewise Polynomials & Splines

- instead of a single polynomial $X$ over its whole domain, use different polynomials in regions defined by **knots**, eg:
$$
y_i = \left\{\begin{align}
\beta_{01} + \beta_{11}x_i + \beta_{21}x_i^2 + \beta_{31}x_i^3 \space \space\space &\text{ if } x_i < c; \\
\beta_{02} + \beta_{12}x_i + \beta_{22}x_i^2 + \beta_{32}x_i^3 \space \space\space &\text{ if } x_i \ge c; \\ 
\end{align}\right.
$$
- better to add constraints to the polynomials, eg continuity
- **splines** have the "maximum" amount of continuity

## 7.2.1 Different types
1. **Piecewise cubic** - nothing special here: just a cubic polynomial fit on the left and right side of the breakpoint 
	- very unlikely to be continuous
2. **Continuous piecewise cubic** - same as above, but enforce continuity at the break point
3. **Cubic spline** - same as above, but also enforce continuous 1st and 2nd derivatives
	- can't go further than 2nd for a *cubic* spline -- if all 3 derivatives are continuous it would just be a polynomial
	- the belief is that the discontinuity in the 3rd derivative is not detectable by the human eye
4. **Linear spline** - linear on the left and right side -- can only enforce continuity at the zeroth degree since this is based on polynomials of degree 1 

## 7.2.2 Linear Splines
A linear spline with knots at $\xi_k, \space k=1, \dots, K$ is a piecewise linear polynomial continuous at each knot

We can represent this model as
$$
y_i = \beta_0 + \beta_1b_1(x_i) + \beta_2b_2(x_i) + \cdots + \beta_{K+3}b_{K+3}(x_i) + \epsilon_i
$$
where the $b_k$ are **basis** functions
$$
\begin{array}{cc}
b_{1}(x_i) = \space x_i & \space \\
\dots & \space\\
b_{k+1}(x_i) = \space (x_i - \xi_k)_+, & k=1, \dots, K 
\end{array}
$$
Here the $()_+$ means the positive part, ie
$$
(x_i - \xi_k)_+ = \left\{ 
\begin{array}{cc}
x_i - \xi_k & \text{if } x_i > \xi_k,\\
0 & \text{otherwise}
\end{array}
\right.
$$

## 7.2.3 Cubic Splines
A cubic spline with knots at $\xi_k, \space k=1, \dots, K$ is a piecewise cubic polynomial with continuous derivatives up to order 2 at each knot.

Again we represent this model with truncated power basis functions:

$$
\begin{align}
y_i =& \beta_0 + \beta_1b_1(x_i) + \cdots + \beta_{K+3}b_{K+3}(x_i) + \epsilon_i, \\
b_1(x_i) =& \space x_i, \\
b_2(x_i) =& \space x_i^2, \\
b_3(x_i) =& \space x_i^3, \\
\dots & \\
b_{K+3}(x_i) =& \space (x_i - \xi_k)_+^3, \space k=1, \dots, K \\
\end{align}
$$

where

$$
(x_i - \xi_k)_+^3 = \left\{ \begin{array}{rr}
(x_i - \xi_k)^3 & \text{ if } x_i > \xi_k, \\
0 & \text{otherwise}
\end{array}
\right.
$$

### Note
Cubic spline has 0th, 1st, 2nd derivatives equal to 0 at the knot

## 7.2.4 Natural Cubic Splines
* A natural cubic spline extrapolates a linear function beyond the boundary knots.
* Adds 4 = 2 * 2 extra constraints to the cubic spline:
	* $f''(\xi_1) = 0$
	* $f''(\xi_K) = 0$
* allows us to put more internal knots for the same degrees of freedom as a regular cubic spline
    * because we don't have to worry about the boundary knots

## 7.2.5 Knot placement

* **Quantile-based** - place knots at quantiles of the data
* **Expert-based** - place knots at known important values
* **Equally-spaced** - place knots at equally-spaced intervals along the range of the data
* **Optimal** - place knots at the points where the RSS is minimized
		* this is a computationally intensive approach
		* can be approximated by a forward stepwise algorithm
* a cubic spline with $K$ knots uses a total of $K+4$ basis functions/degrees of freedom/parameters
* a natural cubic spline with $K$ knots uses a total of $K$ basis functions/degrees of freedom/parameters

# 7.3 Smoothing Splines

Smoothing splines are a way of fitting splines without having to worry about the number or placement of the knots

Criterion for fitting a smooth function $g(x)$ to some data:

$$
\min_{g\in S}\sum_{i=1}^n(y_i - g(x_i))^2 + \lambda\int g''(t)^2dt
$$

* the first term is the RSS
* the second term is a penalty on the roughness of $g$
    * if we didn't have this 2nd term, the solution would be some wild polynomial that goes through every point
* the $\lambda$ is a tuning parameter that controls the roughness penalty
		* $\lambda$ = 0 means we don't care about the roughness penalty
		* $\lambda$ = $\infty$ means we only care about the roughness penalty
		* the smaller $\lambda$ is, the more wiggly the function will be, eventually going through every point
		* the larger $\lambda$ is, the smoother the function will be, eventually being a straight line

## 7.3.1 Solution

* the solution to the above minimization problem is a natural cubic spline with knots at each unique value of $x_i$
* the $\lambda$ controls the wiggliness of the function

## 7.3.2 Details

* smoothing splines avoid the knot selection issue by using all the knots
* there is a single tuning parameter $\lambda$ that controls the amount of wiggliness
* the vector of $n$ fitted values can be written as $\hat{\mathbf{g}}_{\lambda}=\mathbf{S}_{\lambda}\mathbf{y}$ where $\mathbf{S}_{\lambda}$ is an $n\times n$ matrix that depends on $\lambda$
    * this is a **linear smoother**, because it is a linear operator on the $y_i$'s
    * [[3. Linear Regression|Linear regression]] is also a linear operation on $y$
* the **effective degrees of freedom** of a smoothing spline is given by

$$
\begin{align}
df_{\lambda} =& \text{ trace}(\mathbf{S}_{\lambda}) \\
=& \sum_{i=1}^n \left\{ S_{\lambda} \right\}_{ii}
\end{align}
$$

* you might think the degrees of freedom would be $n$ since we have $n$ knots, but the penalty term reduces the effective degrees of freedom
    * a natural adjustment to get effective degrees of freedom is to take the trace of the smoothing matrix
    * when you think about it, it is like a polynomial of degree $n-1$ with $n$ knots, but the degrees of freedom are not all "spent" on the knots

## 7.3.3 Choosing $\lambda$

* because of the nice expression for the effective degrees of freedom, we can specify the degrees of freedom and solve for $\lambda$
* the leave-one-out cross-validation error is given by

$$
\begin{align}	
\text{RSS}_{CV}(\lambda) =& \sum_{i=1}^n(y_i - \hat{g}_{\lambda}^{(-i)}(x_i))^2 \\
=& \sum_{i=1}^n\left(\frac{y_i - \hat{g}_{\lambda}(x_i)}{1 - S_{\lambda ii}}\right)^2
\end{align}
$$

* compute this for a range of $\lambda$ values and choose the one that minimizes the CV error

# 7.4 Local Regression

* **local regression** is a generalization of splines
* with a sliding weight function, we can fit a linear regression model at each of a set of regions by weighted least squares
    * called **loess** (locally estimated scatterplot smoothing)
* the two main ways to do smoothing are splines and local regression  
    * splines are more interpretable
    * local regression is more flexible

# 7.5 Generalized Additive Models

* **generalized additive models** (GAMs) are a generalization of linear models that allow for flexible nonlinearities in several variables, without losing the additivity of a linear model:

$$
y_i = \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \cdots + f_p(x_{ip}) + \epsilon_i
$$

* the $f_j$'s are unspecified smooth functions

## 7.5.1 Details

* can be fit simply using natural cubic splines:

```R
lm(wage ~ ns(age, df=4) + ns(year, df=4) + education, data=Wage)
```

* coefficients are not especially interesting, but the smooth functions are
* can plot the smooth functions using `plot.gam()`
* can mix terms -- some linear, some nonlinear
    * then use `anova()` to compare models and test if the nonlinear terms are significant
* can use smoothing splines or local regression as well:

```R
gam(wage ~ lo(age, span=.5) + s(year, df=4) + education, data=Wage)
```

* GAMs are additive, though low-order interactions can be added in a natural way
    * eg by using bivariate smoothers or interactions of the form $f_1(x_1)f_2(x_2)$

## 7.5.2 GAMs for Classification Problems

$$
\log \left( \frac{p(X)}{1-p(X)} \right) = \beta_0 + f_1(X_1) + f_2(X_2) + \cdots + f_p(X_p)
$$

* when you do a plot, it will plot the contribution of each variable to the log-odds
    * the probabilities are not additive, but the log-odds are