## Often used in:

1. [[Model selection]]
2. [[Anomaly detection]]
3. [[Information theory]] 

[[Kullback–Leibler Divergence is not symmetric]]

- Best for comparing distributions in the same support.
- [[High divergence indicates a poor model fit]]
- [[Low divergence indicates a good fit]]

- [[Kullback–Leibler Divergence]] is useful in #bayesian inference and various machine learning tasks.
- Keep in mind it's not a true #metric, so #triangle-inequality doesn't hold.

## General principle:

- When you're comparing models or distributions, [[Kullback–Leibler Divergence]] can be a powerful tool for understanding how well your model approximates your data.