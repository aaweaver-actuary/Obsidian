# 5.1 cross validation and the Bootstrap
* in this section we discuss two *resampling* methods that are used to assess the performance of a predictive model: **cross validation** and the **bootstrap**
* these methods refit a model of interest to samples formed from the training data, in order to obtain additional information about the fitted model
* for examples, these methods can be used to estimate the test set prediction error
    * standard deviation and bias of the coefficient estimates
    * the relative importance of each predictor
    * the optimal value of the tuning parameter in a penalized model
    * the degree of non-linearity in a non-linear model
    * the possible presence of interactions in a linear model
    * the types of observations that have the largest impact on the fitted model
# 5.2 Training error vs Test error

* the **test** error is the average error that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method
* the **training** error is the average error that results from using a statistical learning method to predict the response on the same observations that were used in training the method
* the training error rate often is quite different from the [[2. Overview of Statistical Learning#2.6 Bias-variance trade-off|test error]], and in particular the former can dramatically underestimate the latter

# 5.3 More on prediction-error estimates
* **best solution:** a large designated test set. However, this is not always possible/available
* some methods make a *mathematical adjustment* to the training error rate in order to estimate the test error rate
    * these include the $C_p$, AIC, BIC, and adjusted $R^2$ statistics
    * discussed later in the course
* here we instead consider a class of methods that estimate the test error by *holding out* a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations

# 5.4 Validation set approach
* here we randomly divide the available set of samples into two parts: a training set and a validation set or hold-out set
* the model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set
* the resulting validation set error rate, typically assessed using MSE in the case of a quantitative response, or misclassification rate in the case of a qualitative response, provides an estimate of the test error rate
## 5.4.1 Drawbacks of validation set approach
* the validation set error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set
* in the validation approach, only a subset of the observations, those that are included in the training set rather than in the validation set, are used to fit the model
    * this suggests that the validation set error rate may tend to *overestimate* the test error rate for the model fit on the entire data set
    
# 5.5 $K$-fold cross validation
* **widely used approach** for estimating test error
* estimates can be used:
    * to select the best model
    * to give an idea of the test error of the final chosen model
* idea is to randomly divide the data into $K$ equal-sized parts, or *folds*
    * we leave out part $k$, fit the model to the other $K-1$ parts, and then obtain predictions for the left-out $k$th part
    * this is done in turn for each part $k = 1, 2, \dots, K$, and then the results are combined
## 5.5.1 the details
* let the $K$ parts be $C_1, C_2, \dots, C_K$
    * $C_k$ denotes the indices of the observations in the $k$th part
    * there are $n_k$ observations in part $k$
        * if $N$ is a multiple of $K$, then $n_k = N/K$ for all $k$
* compute:
$$
\text{CV}_{(K)} = \sum_{k=1}^K \frac{n_k}{n}\cdot \text{MSE}_k
$$
* where $\text{MSE}_k = \sum_{i \in C_k} (y_i - \hat{y}_i)^2 / n_k$
* $\hat{y}_i$ is the fit for observation $i$, obtained from the data with $i$th part removed
* $\text{CV}_{(K)}$ is an estimate of the test MSE, called **cross validation error rate**
* *special case:* $K = N$, then $\text{CV}_{(N)} = \text{MSE}_{\text{LOOCV}}$
    * called **leave-one-out cross validation (LOOCV)**

## 5.5.2 A nice special case!
* with least-squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit!
* the following formula holds:
$$
\text{CV}_{(n)} = \frac{1}{n} \sum_{i=1}^n \left( \frac{y_i - \hat{y}_i}{1 - h_i} \right)^2
$$
* where $\hat{y}_i$ is the $i$th fitted value from the original least squares fit
    * that is, the fit obtained when all of the data are used to fit the model
* $h_i$ is the leverage (the diagonal of the "hat" matrix -- see book for details)
* this is like the normal $\text{MSE}$, but the $i$th residual is divided by $1 - h_i$
* LOOCV is sometimes useful, but typically doesn't shake up the data enough
    * the estimates from each fold are highly correlated
    * hence the average of these estimates has high variance
* a better choice is $K = 5$ or $K = 10$
* so when picking $K$ for cross validation, there is a bias-variance trade-off similar to the one you see in the bias-variance trade-off for the $K$-nearest neighbors classifier
    * higher $K$ means less bias, but higher variance
    * lower $K$ means higher bias, but lower variance
## 5.5.3 Other issues with cross validation
* since each training set is only $(K-1)/K$ as big as the original data set, the estimates of the prediction error will typically be biased upward
* bias is minimized when $K=n$ (aka LOOCV), but variance is high
* $K=5$ or $K=10$ is a good compromise for this bias-variance trade-off
# 5.6 cross validation on classification problems
* the same ideas apply, but the MSE is replaced by the misclassification error rate
* we divide the data into $K$ roughly equal-sized parts $C_1, C_2, \dots, C_K$
* $C_k$ denotes the indices of the observations in the $k$th part
* $n_k$ is the number of observations in the $k$th part
    * if $n$ is a multiple of $K$, then $n_k = n/K$ for all $k$
* compute:
$$
\text{CV}_{(K)} = \sum_{k=1}^K \frac{n_k}{n}\cdot \text{Err}_k
$$
* where $\text{Err}_k = \sum_{i\in C_k} I(y_i \neq \hat{y}_i) / n_k$
* the estimated standard deviation of $\text{CV}_{(K)}$ is given by:
$$
\widehat{\text{SE}} (\text{CV}_{(K)}) = \sqrt{\sum_{k=1}^K \left( \text{Err}_k - \overline{\text{Err}}_{k} \right)^2/(K-1)}
$$
* this is a useful estimate, but strictly speaking, not quite valid
    * the reason is that the $K$ parts overlap, so the errors $\text{Err}_k$ are not independent
    * this means that the formula does not take into account the correlation between the training and validation errors
    * this can be fixed by using the **bootstrap**
* cross validation explicitly separates the training and validation sets
# 5.7 cross validation: right and wrong
### example
* suppose we have $n = 100$ observations, and we have 5000 features to choose from, and want to predict a binary response
    * we decide to take the 100 features that are most strongly correlated with the response
    * throw away the remaining 4900 features
    * fit a logistic regression model using the 100 features
### the wrong way
* how to get an estimate of the test error?
    * can we just use cross validation on the 100 features?
        * no! this is **wrong**
        * this would ignore the fact that we used the entire data set *including the response* to select the 100 features
        * this is itself a form of training and must be included in the cross validation process
    * it is easy to simulate realistic data with the class labels independent of the outcome
        * then true test error is 50%
        * but cross validation on the 100 features will give an estimate of 0% error
* this error happens all the time and has been published in top journals

### the right way
* we must apply cross validation to the entire process of feature selection and model fitting
    * before doing anything, randomly divide the data into $K$ equal-sized parts
    * we can now do whatever we want on the other $K-1$ parts
        * select the 100 features that are most correlated with the response
        * fit a logistic regression model using the 100 features
# 5.8 The bootstrap
* the **bootstrap** is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method
* for example, it can provide an estimate of the standard error of a coefficient, or a confidence interval for that coefficient

## 5.8.1 More about the bootstrap
* in more complex data situations, figuring out the appropriate way to generate bootstrap samples can require some thought
* for example, if the data are time series, we can't simply sample the observations with replacement
    * the data are not independent
    * we expect the value at time $t$ to be correlated to the value at time $t-1$
    * we can use **block bootstrap** to deal with this
        * we divide the data into blocks of consecutive observations
        * we then sample blocks with replacement
    * ultimately we have to sample things that are uncorrelated

## 5.8.2 Other uses of the bootstrap
* primarily used to find standard errors of an estimator
* also provides approximate confidence intervals for a population parameter
    * take the 2.5th and 97.5th percentiles of the bootstrap distribution to obtain a 95% confidence interval
    * this is called the **percentile method**
* **interpretation of a confidence interval:** if we were to repeat the data collection and analysis 100 times, then 95 of the resulting confidence intervals would contain the true unknown value of the parameter

## 5.8.3 Can the bootstrap estimate prediction error?
* in cross validation, each of the $K$ validation folds is distinct from the other $K-1$ folds used for training: **they are non-overlapping**
    * this is critical to the success of cross validation
* to estimate prediction error using the bootstrap, we could think about using each bootstrap dataset as our training set, and the original observations as our test set
    * but this is **wrong**
* each bootstrap sample has significant overlap with the original data
    * about 2/3 of the original data are in each bootstrap sample
    * this means that the bootstrap estimate of prediction error will be far too optimistic

### 5.8.3.1 Removing the overlap
* can partly fix this problem by only using the predictions for those observations that did not (*by chance*) occur in the current bootstrap sample
* this is called the **out-of-bag** (OOB) error estimate
* this gets complicated, and in the end, cross validation provides a simpler, more direct approach to estimating the test error rate
* **the view of the authors:** for this particular problem, the bootstrap is not the best approach
    * cross validation is a better technique for estimating the test error rate