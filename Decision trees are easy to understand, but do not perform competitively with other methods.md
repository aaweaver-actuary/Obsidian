
* #tree based methods are #simple-methods and useful for #interpretability.
* However, they are typically not competitive with the best #supervised-learning approaches in terms of #prediction-accuracy.
* Hence, in this chapter we also describe #bagging, #random-forests, and #boosting.
    * These methods grow multiple trees that are then combined to yield a single consensus prediction.
* Combining a large number of trees can often result in dramatic improvements in #prediction-accuracy at the expense of some loss in #interpretability.

[[Trees are very easy to explain to people.]]
[[Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).]]
[[Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.]]
[[Gradient boosting is the preferred tree-based method]]
[[Random forests]] [[Boosting]] [[Bagging]]
