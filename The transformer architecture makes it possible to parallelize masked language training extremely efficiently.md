#transformer [[BERT is short for Bidirectional Encoder Representations from Transformers|BERT]] [[Natural Language Processing (NLP)]] [[A masked language model enforces bidirectional learning from text by masking a word in a sentence and forcing BERT to bidirectionally use the words on either side of the covered word to predict the masked word|Masked Language Model]]

[[Transformers came from the 2017 paper, Attention is All You Need]]


