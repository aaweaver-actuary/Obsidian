
---
annotation-target: pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf
---

[Summary](https://datasciencetoday.net/index.php/en-us/nlp/211-paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained)



>%%
>```annotation-json
>{"created":"2023-08-24T12:52:14.049Z","updated":"2023-08-24T12:52:14.049Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":161,"end":181},{"type":"TextQuoteSelector","exact":"major breakthroughs ","prefix":"per User21–26 minutesOne of the ","suffix":"in deep learning in 2018 was the"}]}]}
>```
>%%
>*%%PREFIX%%per User21–26 minutesOne of the%%HIGHLIGHT%% ==major breakthroughs== %%POSTFIX%%in deep learning in 2018 was the*
>%%LINK%%[[#^wm00odl2l3|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^wm00odl2l3


>%%
>```annotation-json
>{"created":"2023-08-24T12:52:16.930Z","updated":"2023-08-24T12:52:16.930Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":201,"end":206},{"type":"TextQuoteSelector","exact":"2018 ","prefix":"eakthroughs in deep learning in ","suffix":"was thedevelopment of effective "}]}]}
>```
>%%
>*%%PREFIX%%eakthroughs in deep learning in%%HIGHLIGHT%% ==2018== %%POSTFIX%%was thedevelopment of effective*
>%%LINK%%[[#^vjsnte9tghn|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^vjsnte9tghn


>%%
>```annotation-json
>{"created":"2023-08-24T12:52:22.232Z","updated":"2023-08-24T12:52:22.232Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":228,"end":270},{"type":"TextQuoteSelector","exact":"effective transfer learning methods in NLP","prefix":"g in 2018 was thedevelopment of ","suffix":". Onemethod that took the NLP co"}]}]}
>```
>%%
>*%%PREFIX%%g in 2018 was thedevelopment of%%HIGHLIGHT%% ==effective transfer learning methods in NLP== %%POSTFIX%%. Onemethod that took the NLP co*
>%%LINK%%[[#^bhyvksgxp5w|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^bhyvksgxp5w


>%%
>```annotation-json
>{"target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":848,"end":946},{"type":"TextQuoteSelector","exact":"Language modeling is an effective task for using unlabeled data topre-train neural networks in NLP","prefix":"code examples in PyTorch.TL;DR•","suffix":"• Traditional language models ta"}]}],"created":"2023-08-24T12:53:02.247Z","updated":"2023-08-24T12:53:02.247Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}
>```
>%%
>*%%PREFIX%%code examples in PyTorch.TL;DR•%%HIGHLIGHT%% ==Language modeling is an effective task for using unlabeled data topre-train neural networks in NLP== %%POSTFIX%%• Traditional language models ta*
>%%LINK%%[[#^84xs3p69jd5|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>#nlp
^84xs3p69jd5


>%%
>```annotation-json
>{"created":"2023-08-24T12:55:36.738Z","text":"In contrast, BERT trains a language model that takes **both the previous and next tokens** into account when predicting","updated":"2023-08-24T12:55:36.738Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":948,"end":1028},{"type":"TextQuoteSelector","exact":"Traditional language models take the previous n tokens andpredict the next one. ","prefix":"e-train neural networks in NLP• ","suffix":"In contrast, BERT trains a langu"}]}]}
>```
>%%
>*%%PREFIX%%e-train neural networks in NLP•%%HIGHLIGHT%% ==Traditional language models take the previous n tokens andpredict the next one.== %%POSTFIX%%In contrast, BERT trains a langu*
>%%LINK%%[[#^p0e9wu7ts8a|show annotation]]
>%%COMMENT%%
>In contrast, BERT trains a language model that takes **both the previous and next tokens** into account when predicting
>%%TAGS%%
>#bert
^p0e9wu7ts8a


>%%
>```annotation-json
>{"created":"2023-08-24T12:57:41.732Z","updated":"2023-08-24T12:57:41.732Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":1306,"end":1476},{"type":"TextQuoteSelector","exact":"BERT is also trained on a next sentence prediction task to betterhandle tasks that require reasoning about the relationship betweentwo sentences (e.g. question answering)","prefix":"s into account whenpredicting.• ","suffix":"• BERT uses the Transformer arch"}]}]}
>```
>%%
>*%%PREFIX%%s into account whenpredicting.•%%HIGHLIGHT%% ==BERT is also trained on a next sentence prediction task to betterhandle tasks that require reasoning about the relationship betweentwo sentences (e.g. question answering)== %%POSTFIX%%• BERT uses the Transformer arch*
>%%LINK%%[[#^lkv2aejg13|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^lkv2aejg13


>%%
>```annotation-json
>{"created":"2023-08-24T12:57:55.474Z","updated":"2023-08-24T12:57:55.474Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":1478,"end":1540},{"type":"TextQuoteSelector","exact":"BERT uses the Transformer architecture for encoding sentences.","prefix":"nces (e.g. question answering)• ","suffix":"• BERT performs better when give"}]}]}
>```
>%%
>*%%PREFIX%%nces (e.g. question answering)•%%HIGHLIGHT%% ==BERT uses the Transformer architecture for encoding sentences.== %%POSTFIX%%• BERT performs better when give*
>%%LINK%%[[#^bwiciffxut7|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>#bert, #transformer
^bwiciffxut7


>%%
>```annotation-json
>{"created":"2023-08-24T12:58:18.416Z","updated":"2023-08-24T12:58:18.416Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":1542,"end":1613},{"type":"TextQuoteSelector","exact":"BERT performs better when given more parameters, even onsmall datasets.","prefix":"ecture for encoding sentences.• ","suffix":"Transfer Learning in NLPIf you’r"}]}]}
>```
>%%
>*%%PREFIX%%ecture for encoding sentences.•%%HIGHLIGHT%% ==BERT performs better when given more parameters, even onsmall datasets.== %%POSTFIX%%Transfer Learning in NLPIf you’r*
>%%LINK%%[[#^cqfo857ihyd|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^cqfo857ihyd


>%%
>```annotation-json
>{"created":"2023-08-24T12:58:28.963Z","updated":"2023-08-24T12:58:28.963Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":1683,"end":1700},{"type":"TextQuoteSelector","exact":"transfer learning","prefix":"ady familiar with the basics of ","suffix":" in NLPand just want to learn mo"}]}]}
>```
>%%
>*%%PREFIX%%ady familiar with the basics of%%HIGHLIGHT%% ==transfer learning== %%POSTFIX%%in NLPand just want to learn mo*
>%%LINK%%[[#^efe7huh20v7|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^efe7huh20v7


>%%
>```annotation-json
>{"created":"2023-08-24T12:58:38.761Z","updated":"2023-08-24T12:58:38.761Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":1800,"end":1807},{"type":"TextQuoteSelector","exact":"Before ","prefix":"nskip ahead to the next section.","suffix":"methods like ELMo and BERT, pret"}]}]}
>```
>%%
>*%%PREFIX%%nskip ahead to the next section.%%HIGHLIGHT%% ==Before== %%POSTFIX%%methods like ELMo and BERT, pret*
>%%LINK%%[[#^gimsuld7wnb|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^gimsuld7wnb


>%%
>```annotation-json
>{"created":"2023-08-24T12:59:27.585Z","text":"map $\\text{word} \\rightarrow \\mathbb{R}^p$","updated":"2023-08-24T12:59:27.585Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":1912,"end":2000},{"type":"TextQuoteSelector","exact":"Wordembeddings mapped each word to a vector that represented someaspects of its meaning ","prefix":"ngs such as word2vec and GloVe. ","suffix":"(e.g. the vector for “King” woul"}]}]}
>```
>%%
>*%%PREFIX%%ngs such as word2vec and GloVe.%%HIGHLIGHT%% ==Wordembeddings mapped each word to a vector that represented someaspects of its meaning== %%POSTFIX%%(e.g. the vector for “King” woul*
>%%LINK%%[[#^t0q2kd5ytn8|show annotation]]
>%%COMMENT%%
>map $\text{word} \rightarrow \mathbb{R}^p$
>%%TAGS%%
>#embedding, #semantic-meaning
^t0q2kd5ytn8


>%%
>```annotation-json
>{"created":"2023-08-24T12:58:47.252Z","text":"like word2vec, GloVe","updated":"2023-08-24T12:58:47.252Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":1835,"end":1884},{"type":"TextQuoteSelector","exact":"pretraining in NLP waslimited to word embeddings ","prefix":"ore methods like ELMo and BERT, ","suffix":"such as word2vec and GloVe. Word"}]}]}
>```
>%%
>*%%PREFIX%%ore methods like ELMo and BERT,%%HIGHLIGHT%% ==pretraining in NLP waslimited to word embeddings== %%POSTFIX%%such as word2vec and GloVe. Word*
>%%LINK%%[[#^iivq5u6s3id|show annotation]]
>%%COMMENT%%
>like word2vec, GloVe
>%%TAGS%%
>#embedding, #nlp
^iivq5u6s3id


>%%
>```annotation-json
>{"created":"2023-08-24T13:01:13.097Z","updated":"2023-08-24T13:01:13.097Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":2082,"end":2147},{"type":"TextQuoteSelector","exact":"Word embeddings aregenerally trained on large, unlabeled corpora ","prefix":"on about status, gender, etc.). ","suffix":"(such as theWikipedia dump), and"}]}]}
>```
>%%
>*%%PREFIX%%on about status, gender, etc.).%%HIGHLIGHT%% ==Word embeddings aregenerally trained on large, unlabeled corpora== %%POSTFIX%%(such as theWikipedia dump), and*
>%%LINK%%[[#^72i99zyju4w|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^72i99zyju4w


>%%
>```annotation-json
>{"created":"2023-08-24T13:02:26.618Z","updated":"2023-08-24T13:02:26.618Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":2948,"end":3062},{"type":"TextQuoteSelector","exact":"language models of methods likeword2vec have trouble capturing the meaning of combinations ofwords, negation, etc.","prefix":"othercomplex architectures, the ","suffix":"Another key limitation is that w"}]}]}
>```
>%%
>*%%PREFIX%%othercomplex architectures, the%%HIGHLIGHT%% ==language models of methods likeword2vec have trouble capturing the meaning of combinations ofwords, negation, etc.== %%POSTFIX%%Another key limitation is that w*
>%%LINK%%[[#^kqv0d28jzq|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^kqv0d28jzq


>%%
>```annotation-json
>{"created":"2023-08-24T13:01:48.228Z","updated":"2023-08-24T13:01:48.228Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":2516,"end":2592},{"type":"TextQuoteSelector","exact":"One limitation is that word embedding models are generally notvery powerful.","prefix":"mitations to this simple method.","suffix":" Word2vec and fasttext are both "}]}]}
>```
>%%
>*%%PREFIX%%mitations to this simple method.%%HIGHLIGHT%% ==One limitation is that word embedding models are generally notvery powerful.== %%POSTFIX%%Word2vec and fasttext are both*
>%%LINK%%[[#^pig04rvacdh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>#embedding, #limitation
^pig04rvacdh


>%%
>```annotation-json
>{"created":"2023-08-24T13:02:45.835Z","updated":"2023-08-24T13:02:45.835Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":3062,"end":3085},{"type":"TextQuoteSelector","exact":"Another key limitation ","prefix":"inations ofwords, negation, etc.","suffix":"is that word embedding models do"}]}]}
>```
>%%
>*%%PREFIX%%inations ofwords, negation, etc.%%HIGHLIGHT%% ==Another key limitation== %%POSTFIX%%is that word embedding models do*
>%%LINK%%[[#^1rfrrg29ox|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^1rfrrg29ox


>%%
>```annotation-json
>{"created":"2023-08-24T13:01:20.050Z","updated":"2023-08-24T13:01:20.050Z","document":{"title":"paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","link":[{"href":"urn:x-pdf:153ad1ddf0ae548d9d3f6ecb6e0c216d"},{"href":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf"}],"documentFingerprint":"153ad1ddf0ae548d9d3f6ecb6e0c216d"},"uri":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","target":[{"source":"vault:/pdf/paper-dissected-BERT-pretraining-of-deep-bidirectional-transformers-for-language-understanding-explained.pdf","selector":[{"type":"TextPositionSelector","start":2225,"end":2269},{"type":"TextQuoteSelector","exact":"downstream tasks such as sentiment analysis.","prefix":"train models on labeled datafor ","suffix":" This allows thedownstream model"}]}]}
>```
>%%
>*%%PREFIX%%train models on labeled datafor%%HIGHLIGHT%% ==downstream tasks such as sentiment analysis.== %%POSTFIX%%This allows thedownstream model*
>%%LINK%%[[#^ndmhuo08u0m|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>#downstream, #sentiment-analysis
^ndmhuo08u0m
